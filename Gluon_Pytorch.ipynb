{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Eric Steele, Liam Tiemon, Mate Virag\n",
    "### Variant 2 - Analysis and Comparison of Gluon, PyTorch and Tensorflow\n",
    "May 4, 2018  \n",
    "Dr. Kevin Kirby  \n",
    "CSC 494"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation \n",
    "___\n",
    "\n",
    "### Gluon\n",
    "___\n",
    "\n",
    "#### For Mac:\n",
    "1. Go to terminal\n",
    "    * Optional: create a new virtual environment\n",
    "2. Run these commands\n",
    "    * pip install gluon\n",
    "    * pip install mxnet\n",
    "\n",
    "#### For Windows:\n",
    "1. Go to http://landinghub.visualstudio.com/visual-cpp-build-tools and download and install the C++ compiler.\n",
    "2. Go to Anaconda prompt\n",
    "    * Optional: create a new virtual environment\n",
    "3. Run these commands\n",
    "    * pip install gluon\n",
    "    * pip install mxnet\n",
    "    \n",
    "___\n",
    "\n",
    "### PyTorch\n",
    "___\n",
    "#### For Mac:\n",
    "1. Go to terminal\n",
    "    * Optional: create a new virtual environment\n",
    "2. Go to PyTorch's website (http://pytorch.org) and specify your desired configuration\n",
    "3. Run the returned pip or conda command to install PyTorch\n",
    "\n",
    "#### For Windows:\n",
    "1. Go to Anaconda prompt\n",
    "    * Optional: create a new virtual environment\n",
    "2. Go to PyTorch's website (http://pytorch.org) and specify your desired configuration\n",
    "3. Run the returned pip or conda command to install PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation \n",
    "\n",
    "---\n",
    "\n",
    "## Gluon\n",
    "\n",
    "* http://gluon.mxnet.io contains most of the information required to get started, had code examples, and good documentation.\n",
    "    * This helped us get our project up and going. Following their tutorial helped us get our CNN started and from there we were able to change it to our liking. \n",
    "* http://mxnet.incubator.apache.org/api/python/index.html contains tutorials and documentation for APIs.\n",
    "    * It's API documentation helped us determine which APIs were needed for layers in our CNN.\n",
    "    \n",
    "## PyTorch\n",
    "\n",
    "* There isn't much PyTorch documentation besides the base documentation from the developers and a few GitHub repositories.\n",
    "* PyTorch's [website](https://pytorch.org/tutorials/index.html) has enough code examples to get you started, but not enough to get you in a good place with a CNN\n",
    "* https://github.com/utkuozbulak/pytorch-custom-dataset-examples\n",
    "    * This GitHub repository was a huge help in getting NkuMyaDevMaker.py up and running with PyTorch's network.\n",
    "* https://github.com/pytorch/examples\n",
    "    * The official PyTorch GitHub repository was useful to implement layer connections, defining the network, making drop out, and how to use the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ease of Use\n",
    "___\n",
    "\n",
    "## Network\n",
    "\n",
    "___\n",
    "\n",
    "#### Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports necessary\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, ndarray\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the model\n",
    "net = gluon.nn.Sequential()\n",
    "    \n",
    "# Declare hyperparameters\n",
    "convo1_kernels = 20\n",
    "convo1_kernel_size = (5,5)\n",
    "convo2_kernels = 40\n",
    "convo2_kernel_size = (5,5)\n",
    "pooling = 2\n",
    "\n",
    "hidden1_neurons = 20\n",
    "dropout_rate = 0.3\n",
    "hidden2_neurons = 15\n",
    "\n",
    "# Define our network\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=convo1_kernels, kernel_size=convo1_kernel_size, use_bias=True, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=pooling, strides=pooling))\n",
    "    net.add(gluon.nn.BatchNorm())\n",
    "    net.add(gluon.nn.Conv2D(channels=convo2_kernels, kernel_size=convo2_kernel_size, use_bias=True, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=pooling, strides=pooling))\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(hidden1_neurons, activation=\"relu\", use_bias=True))\n",
    "    net.add(gluon.nn.Dropout(dropout_rate))\n",
    "    net.add(gluon.nn.Dense(hidden2_neurons, activation=\"relu\", use_bias=True))\n",
    "    net.add(gluon.nn.Dense(1, activation=\"sigmoid\", use_bias=True)) # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a network in Gluon is similar to Tensorflow. It offers the same types of convolutional and dense layers with very similar lists of parameters that can be passed in to them. It also offers features, such as batch normalization, dropout and image flattening just like Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports used\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\"\"\"Class that defines the neural network.\"\"\"\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Defines the layers in the neural network.\"\"\"\n",
    "    def __init__(self, depth, nk, kernel_size, padding, hidden_neurons, nc):\n",
    "        super(Net, self).__init__()\n",
    "        # out_channels defines the number of kernels\n",
    "        self.conv1 = nn.Conv2d(in_channels=depth, out_channels=nk[0], kernel_size=kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels=nk[0], out_channels=nk[1], kernel_size=kernel_size)\n",
    "        #self.conv2_drop = nn.Dropout2d()\n",
    "        # nc is the image size after convolution and pooling\n",
    "        self.fc1 = nn.Linear(nc * nc * nk[1], hidden_neurons[0])\n",
    "        self.fc2 = nn.Linear(hidden_neurons[0], hidden_neurons[1])\n",
    "        # Single value output\n",
    "        self.fc3 = nn.Linear(hidden_neurons[1], 1)\n",
    "\n",
    "    \"\"\"\n",
    "    Defines the connections and the activation functions between layers, pushes the \n",
    "    input patterns through the network and returns the network's output.\n",
    "    \"\"\"\n",
    "    def forward(self, x, pooling):\n",
    "        # Max pooling over a square window with stride of pool size to avoid overlaps\n",
    "        # Activatin functions are specified in this function even for the convolutional layer\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=pooling, stride=pooling)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=pooling, stride=pooling)\n",
    "        #x = F.max_pool2d(F.relu(self.conv2_drop(self.conv2(x))), kernel_size=pooling, stride=pooling)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the size of the flat array for the input of the first dense layer \n",
    "    after the last convolutional layer.\n",
    "    \"\"\"\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # All dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a network in PyTorch is a lot different than in Gluon or Tensorflow because it is a low-level framework compared to the other two frameworks we used. PyTorch requires a class inheriting from torch.nn.Module to be implemented when creating a neural network. This class is generally implemented using three functions that define the network. The \\__init\\__ function defines the layers in the neural network, however, PyTorch defines these layers at a much lower level than Tensorflow or Gluon. For example, PyTorch requires the user to calculate the number of input and output channels for each layer, while Tensorflow and Gluon automatically handled those calculations. Furthermore, PyTorch doesn't allow the user to define the activation function for the layers, instead, it requires the user to push the activations through an activation function manually prior to passing the output as the input of the next layer.\n",
    "\n",
    "### Built in\n",
    "___\n",
    "\n",
    "#### Gluon\n",
    "* Batching was automatically done based on a parameter passed onto the data loader\n",
    "* Gluon includes a wide variety of pre implemented basic, convolutional, pooling and activation layers varying from 1D to 3D\n",
    "* Input channels are automatically computed between layers.\n",
    "* Batch Normalization and Drop Out functions are pre implemented.\n",
    "* Flatten image calculations are automatically done.\n",
    "* You can pass in activation functions, much like tensorflow\n",
    "* Parameter specification into methods had a wide range of options.\n",
    "\n",
    "#### PyTorch\n",
    "* Low level compared to Tensorflow and Gluon\n",
    "* Batching was automatically done based on a parameter passed onto the data loader\n",
    "\n",
    "### Implemented\n",
    "___\n",
    "\n",
    "#### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Accuracy function for a two-class classifier. Receieves real numbers where one class\n",
    "is associated with 0.0 and the other with 1.0. A prediction within 0.33 of the\n",
    "label is considered a correct result. The function returns the number of\n",
    "correct classifications across a batch of predictions and labels.\n",
    "\"\"\"\n",
    "def accuracy(predictions, labels):\n",
    "    # Convert mxnet NDArrays to numpy NDArrays\n",
    "    pred = predictions.asnumpy()[:,0]\n",
    "    lab = labels.asnumpy()[:,0]\n",
    "    correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        if abs(pred[i] - lab[i]) < 0.33:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When attempting to design our own network in Gluon, we initially ran into difficulty with the accuracy function. The built-in version mx.metric.Accuracy is intended for use with one-hot outputs, but we designed our network to use a single output to match the design of the TensorFlow network from HW4. This discrepancy caused our result to consistently be 50% accuracy. Once we identified this issue we wrote our own accuracy function to match the formula used in TfCnn-MyaDev_For_HW4.py, we identified that the networks were in fact learning. We were able to use this function with both Gluon and PyTorch.\n",
    "\n",
    "#### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "class MyaDevDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.X = X                  # NkuMyaDevMaker images\n",
    "        self.Y = Y                  # NkuMyaDevMaker labels\n",
    "        self.transform = transform  # Transformation function (optional)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = (self.X[idx], self.Y[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially tried to use Numpy arrays to pass our training and test images to the DataLoader class. We had difficulty identifying the correct shape for that array and elected to write a version of the Dataset class instead. Dataset is very straightforward. The data can be passed to the \\__init\\__ in any form, the \\__len\\__ function returns the number of elements in the dataset, and the \\__getitem\\__ function returns a single element by index (optionally with a transformation applied).\n",
    "\n",
    "This Dataset is then passed to the DataLoader class along with a batch size. The DataLoader is iterable and handles minibatching, returning tuples of data and labels from the Dataset, split into the specified batch size.\n",
    "\n",
    "Gluon and PyTorch use \n",
    "\n",
    "#### PyTorch\n",
    "* A class had to be created in order to specify connections between layers and hand made calculations had to be done when inputted into a flatten or max pooling layer.\n",
    "\n",
    "# Results\n",
    "\n",
    "## Reimplementing TensorFlow Network\n",
    "\n",
    "Running a Gluon implementation of the network from TfCnn-MyaDev_For_HW4.py gave results consistently around 95% accuracy with the default hyperparameters provided in the file. This is roughly consistent with the results of running the network in TensorFlow.\n",
    "\n",
    "The PyTorch implementation of the TfCnn-MyaDev_For_HW4.py network got around 94% accuracy when successful, but approximately one fifth of the time does not improve.\n",
    "\n",
    "## Designing New Networks\n",
    "\n",
    "For our own network design in Gluon, we initially tried using SGD as our training algorithm and Mean Squared Error for loss. This was effective but took a long time to produce results. In an effort to make the network learn more quickly we added a batch normalization layer, second convolution layer, dropout layer, and a second hidden layer. There was some improvement, but training was still slow. We then tried to the Adam training algorithm instead and found that it trained very quickly, hitting its accuracy threshold and stopping almost immediately. We changed the network to run for the maximum number of epochs and found that rather than overfitting the network consistently hit accuracies of above 98%.\n",
    "\n",
    "In PyTorch we started with Adam as the training algorithm. However we began with Mean Absolute Error, mistakenly thinking that PyTorch did not offer Mean Squared Error because the PyTorch version of that function was named differently than in Gluon. Once we identified this we switched to MSE and got significantly better results. We also gradually increased the training set size which also helped. Our final results achieved around 97% accuracy, although that rate fluctuated more than that of our Gluon network.\n",
    "\n",
    "Across both the existing network and our designs, PyTorch consistently had less reliable and consistent results than Gluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
